{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90790d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, \\\n",
    "f1_score, log_loss\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification,  \\\n",
    "Trainer, TrainingArguments\n",
    "\n",
    "import evaluate\n",
    "from scipy.special import softmax\n",
    "import warnings\n",
    "from warnings import simplefilter\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bd3f1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"../config/config.yaml\"\n",
    "config = yaml.load(open(config_path), Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dcf05aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = config['preprocessing']\n",
    "train_params = config['train']\n",
    "test_params = config['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11325770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path: str):\n",
    "    \n",
    "    data = pd.read_csv(path)\n",
    "    cols_to_drop = data.columns[data.columns.str.startswith('Unnamed')]\n",
    "    \n",
    "    data.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    data = data.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def plotting_trainer_loss(trainer):\n",
    "    auc = []\n",
    "    eval_loss = []\n",
    "    \n",
    "    \n",
    "    for step in trainer.state.log_history:\n",
    "        try:\n",
    "            auc.append(step[\"eval_roc_auc\"])\n",
    "            eval_loss.append(step['eval_loss'])\n",
    "            \n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "    sns.lineplot(eval_loss, ax=axes[0], color='orange')\n",
    "    sns.lineplot(auc, ax=axes[1])\n",
    "    \n",
    "    axes[0].set_title('Validation Loss')\n",
    "    axes[1].set_title('ROC-AUC SCORE')\n",
    "    axes[0].set(xlabel='Epochs')\n",
    "    axes[1].set(xlabel='Epochs')\n",
    "    \n",
    "    \n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Расчет метрики roc-auc\"\"\"\n",
    "\n",
    "    metric = evaluate.load(\"roc_auc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = softmax(logits)[:, 1]\n",
    "    res = metric.compute(prediction_scores=predictions, \n",
    "                         references=labels)\n",
    "\n",
    "    return {'roc_auc': res['roc_auc']}\n",
    "\n",
    "\n",
    "def get_metrics(y_test: np.array, y_pred: np.array, y_proba: np.array) -> Dict:\n",
    "    dict_metrics = {\n",
    "        'roc_auc': round(roc_auc_score(y_test, y_proba[:, 1]), 3),\n",
    "        'precision': round(precision_score(y_test, y_pred), 3),\n",
    "        'recall': round(recall_score(y_test, y_pred), 3),\n",
    "        'f1': round(f1_score(y_test, y_pred), 3),\n",
    "        'logloss': round(log_loss(y_test, y_proba), 3)\n",
    "    }\n",
    "\n",
    "    return dict_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f13ce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData:\n",
    "    \n",
    "    def __init__(self, texts, tokenizer, batch_size_split=train_params['batch_size_split'], \n",
    "                 max_length=train_params['max_length']):\n",
    "        \n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size_split = batch_size_split\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def pre_tokenizer(self, text):\n",
    "        return self.tokenizer(text,\n",
    "                              add_special_tokens=True, \n",
    "                              max_length=self.max_length,\n",
    "                              pad_to_max_length=True,\n",
    "                              truncation=True,\n",
    "                              return_attention_mask=True,\n",
    "                              return_tensors='pt')\n",
    "    \n",
    "        \n",
    "    def transform(self):\n",
    "        \n",
    "        N = len(self.texts)\n",
    "        size_split = N // self.batch_size_split\n",
    "\n",
    "        train_encodings = self.pre_tokenizer(self.texts[:size_split])\n",
    "        input_ids = train_encodings['input_ids']\n",
    "        attention_mask = train_encodings['attention_mask']\n",
    "        token_type_ids = train_encodings['token_type_ids']\n",
    "\n",
    "        for pos in tqdm(range(size_split, N, size_split)):\n",
    "            train_encodings_2 = self.pre_tokenizer(self.texts[pos:pos +\n",
    "                                                              size_split])\n",
    "            input_ids = torch.cat((input_ids, train_encodings_2['input_ids']))\n",
    "            attention_mask = torch.cat(\n",
    "                (attention_mask, train_encodings_2['attention_mask']))\n",
    "            token_type_ids = torch.cat(\n",
    "                (token_type_ids, train_encodings_2['token_type_ids']))\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'token_type_ids': token_type_ids\n",
    "        } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bc669c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8042c63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random_state': 10,\n",
       " 'test_size': 0.25,\n",
       " 'raw_path': '../data/raw/data_reviews.csv',\n",
       " 'train_path': '../data/processed/train_data.csv',\n",
       " 'test_path': '../data/processed/test_data.csv'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3fa8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = get_dataset(preproc['test_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74a8283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(train_params['tokenizer_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7e2483f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5450368e871644a39a4d3083838ca41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf = PrepareData(test_df.reviewText.tolist(), tokenizer)\n",
    "test_encodings = clf.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2f1a64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test_encodings, test_df.target.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb0ca031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_length': 512,\n",
       " 'batch_size': 8,\n",
       " 'random_state': 10,\n",
       " 'learning_rate': 2e-05,\n",
       " 'tokenizer_path': 'cointegrated/rubert-tiny2',\n",
       " 'model_path': 'cointegrated/rubert-tiny2',\n",
       " 'epochs': 10,\n",
       " 'weight_decay': 0.01,\n",
       " 'per_device_batch_size': 64,\n",
       " 'batch_size_split': 10,\n",
       " 'metrics_path': '../metrics/metrics.json'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37305897",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(test_params['model_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01cc7773",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Предсказание модели классификации Bert\n",
    "y_pred = trainer.predict(test_dataset)\n",
    "\n",
    "# Получение вероятностей\n",
    "pred_proba = y_pred[0]\n",
    "\n",
    "# Получение предсказанных меток класса\n",
    "pred = pred_proba.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0bd2aff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'roc_auc': 0.973,\n",
       " 'precision': 0.924,\n",
       " 'recall': 0.898,\n",
       " 'f1': 0.911,\n",
       " 'logloss': 1.021}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = get_metrics(test_df.target.tolist(), pred, pred_proba)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7b0db152",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (train_params['metrics_path'], 'a') as f:\n",
    "    json.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a4d539e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../config/config.yaml\", 'w',) as f :\n",
    "#     yaml.dump(config, f, sort_keys=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
